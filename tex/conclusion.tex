\chapter{Conclusion} % (fold)
\label{cha:conclusion}

In this thesis, a probabilistic model for transforming a voice to sound like another specific voice has been tested. The model is fully automatic and only requires some 100 training sentences from both speakers with the same acoustic content. The classical source-filter decomposition allows prosodic and spectral transformation to be performed independently. The transformations are based on a Gaussian mixture model and a transformation function suggested by Y. Stylianou \cite{stylianou98}. Feature vectors of the same content from the source and target speaker, aligned in time by dynamic time warping, were fitted to the GMM. The short time spectra, represented as cepstral coefficients and derived from LPC \cite{atal68}, and the pitch periods, represented as fundamental frequency estimated from the RAPT algorithm \cite{talkin95}, were transformed with the same probabilistic transformation function.

An important part of the training procedure is the time alignment by dynamic time warping (DTW). The DTW was configured to not alter the source feature vectors, to get a discrete mapping of every source vector to a target vector. The global constraints were set to guarantee a maximum modification of the target vectors and the local constraints were optimised for the Itakura distance \cite{itakura75min}.

Time aligned feature vectors were fitted to a 128 mixture GMM with diagonal covariance matrices, used in the transformation function. The cross-correlation matrix $\mathbf{\Sigma}^{yx}$, which is not available from the diagonal GMM, was estimated from training data. When both input and output of the transformation is known, the only unknown parameter is the cross-correlation matrix, which could be least-square optimised by the normal equations \cite{strang06}. Pre-emphasis \cite{turk06} and exclusion of the cepstral energy coefficient $c_0$ \cite{stylianou98} was tested in the filter transform, but did not enhance the transformation quality. The transform was tested on 10 sentences and revealed an over-smoothed spectrum but with small mean deviation from the target spectrum. The fundamental frequency was transformed by a 64-mixture full-matrix GMM with different configurations of the training vector, and tested on the same 10 sentences. Objective results reviled a smaller standard deviation than a simple scaling of the source pitch. 

It is not easy to compare the quality of the transformations to other works because the different articles tend to use different metrics and even different implementations of the same metrics \cite{stylianou98,najjary04,ye06,kim97}. However, the frequency transform is pretty straightforward, so any deviation from other's result would mainly be due to different training and test sets, or implementation errors. The main novel approach in this thesis is the use of the transformation function in the fundamental frequency transform.

The transformation function has the property of finding remaining elements of a vector given a new incomplete vector. Several such vector combinations were tested in the pitch transform. Given that the cepstrum transform was perfect, the best pitch (fundamental frequency) transformation was by using the transformed cepstrum as input. As this was not the case, the best transformation of the fundamental frequency was by mapping the source fundamental frequency to the target. The fundamental frequency contour was smoothed by a novel ``modified moving average'' algorithm. In spite of the fact that the modified moving average algorithm uses temporal information from consecutive transformation, which the GMM does not utilise, it performed better than a normal moving average filter.

A listening test was performed with the best setup from objective tests and the results indicate that it is possible to recognise the transformed voice as the target speaker with a 72 \% probability. However, it was a pretty basic test where the identification options were limited to only the source and target speaker. The synthesised voice was affected by a muffling effect due to incorrect frequency transformation and the prosody sounded somewhat robotic. There is clearly still some work to be done before the voice transformation could be used in commercial product.


\section{Future Work} % (fold)
\label{sec:future_work}

The major shortcoming of the presented implementation is the lack of time-scale modifications. Although there is some scaling in the PSOLA synthesis where the pitch period is modified, the pitch cycles are not discarded or duplicated. If the source and target speaker has a significant different speaking pace, this yields a problem in the speaker identification, verified in the listening test. A mapping rule for discarding or duplicating pitch cycles could be found in the DTW mapping where one vector is mapped to several vectors. However, if the goal is to impersonate a target speaker this would not be a problem. Energy transformation to modify the perceived loudness of in speech would also make the transformation more convincing. While this somewhat implemented by transforming the $c_0$ coefficient with questionable success, it could be implemented as part of the excitation modification as well. 

The increase to correlation of consecutive transformed fundamental frequencies, the GMM could be trained by a set of $f_0$ parameters corresponding to a set of cepstral vectors, $\mathbf{Z}=\sbrackets{\mathbf{y}_{cc}^1,\mathbf{y}_{cc}^2,\dotsc,\mathbf{y}_{cc}^n,f_{0}^1,f_{0}^2,\dotsc,f_{0}^n}$ yielding a many-to-many mapping instead of one-to-one, as suggested by \cite{najjary04}. The complexity of the pitch transform would increase by many orders of magnitudes, but the transformed $f_0$ contour might be more natural. One could also use a model that utilises temporal dependency information \cite{kim97}, which the GMM does not.

The excitation from the inverse LP filtering is far from white noise. By listening to the excitation signal the content of the original speech signal could be recognised. Instead of using the excitation from the source speaker in the synthesis, a small database of excitation signals from the target speaker could be used with the same transformation function to map the excitation signal to one from the target speaker. The excitation signal must represented in regular time domain which yields vectors of approximately 80 sample for a 10 ms frame and a sample frequency of 8 kHz. By choosing a small GMM, with \eg 16 mixtures, to computational time would be acceptable.

The frequency transform would benefit from more mixtures in the GMM, but again, that would require more training data, which might be a problem. E. Helander \cite{helander08} suggested a method based on LSF feature vectors instead of cepstral coefficients which might cope with the training data issue.

% section Further Work (end)

% chapter Conclusion (end)