\chapter{Theory} % (fold)
\label{cha:theory}

The basic idea of the voice transformation is to make a linear prediction, LP, analysis of the signal and extract the estimate from the real signal to yield a source-filter separation, where the excitation represents the source and the LP coefficients represent the filter. The frequency spectra of the voice can be transformed by altering the filter coefficients to match the new voice, and the pitch can be transformed by performing the PSOLA technique on the excitation signal. The transformed excitation and LP coefficients are used in the inverse filter to synthesis the transformed voice, as depicted in Figure~\ref{fig:transformation_system}.
\begin{figure}[htbp]
  \begin{center}
  \begin{tabular}[h]{c}
	\xymatrix{ 
	\mathbf{x} \ar[rr] & \ar[d]\ar[r] &*+<5mm>[F-,]{1-A(z)}  \ar[r]^<(0.3){\mathbf{e}} &*+<5mm>[F-,]{\txt{PT}} \ar[r]^<(0.3){\tilde{\mathbf{e}}}  &*+<5mm>[F-,]{\frac{1}{1-\tilde{A}(z)}} \ar[r] & \tilde{\mathbf{y}} \\  % end line 1
	&*+<5mm>[F-,]{\txt{LPC}} \ar[rr]^<(0.25){A(z)} & \ar[u]&*+<5mm>[F-,]{\txt{FT}}\ar `[ru]^<(0.5){\tilde{A}(z)} [ur]&  &}
  \end{tabular}
  \caption{Voice transformation system.}
  \label{fig:transformation_system}
  \end{center}
\end{figure}
\abbrev{FT}{Filter Transform} \abbrev{PT}{Pitch Transform}

The challenge in this procedure is to find global context-independent transformation functions explained in Section~\ref{sec:transformation_function}. The transformation function is trained with known corresponding source and target vectors to output the correct target vector with a new source vector as input. In the training process the source and target speaker signals are time-aligned with dynamic time warping, Section~\ref{sec:dynamic_time_warping}. The corresponding vectors are represented as Gaussian mixture models (GMM), Section~\ref{sec:gaussian_mixture_model}.


% \begin{figure}[htbp]
% 	\centering
% 	\begin{tabular}[h]{c}
% 		\xymatrix{ 
% \mathbf{x} \ar[r] &*+<5mm>[F-,]{\txt{LPC}}\ar[d]\ar @{=>}[r] &*+<5mm>[F-,]{\txt{GMM}}\ar `[rd]^{\boldsymbol{\Phi}} [dr] &\\ % end line 1
% &*+<5mm>[F-,]{\txt{DTW}} \ar @<3pt>[rr]^<(0.15){\mathbf{x}} \ar @<-3pt>[rr]_<(0.15){\tilde{\mathbf{y}}} & &*+<5mm>[F-,]{\txt{FT training}}\\ % end line 2
% \mathbf{y} \ar[r] &*+<5mm>[F-,]{\txt{LPC}}\ar[u]&&}
% 	\end{tabular}
% 	\caption{Training of filter transformation parameters.}
% 	\label{fig:VC_training}
% \end{figure}
% In the training process of the transformation functions the two input signals, source $x$ and target $y$, first need to be aligned in time, Section~\ref{sec:dynamic_time_warping}. The source data is classified into probabilistic Gaussian mixture models (GMM), Section~\ref{sec:gaussian_mixture_model}, and used in a class-specific transform \cite{stylianou09}. The parameters in the transformation function are derived from a set of source vectors and corresponding time aligned target vectors, and a GMM fitted to the source.

\section{Signal Representation} % (fold)
\label{the:signal_representation}
Representing a signal in the time domain requires a huge amount of storage, \eg representing 10 ms with a sampling frequency of 8 kHz yields 80 values and a whole sentence would need approximately 40 000 values. Linear predictive coding, Section~\ref{sub:linear_predictive_coding}, is a fast and simple signal representation which only requires \eg 10 values to represent a signal segment of 10 ms, as opposed to 80. The LPC coefficients, $A(z)$, are the pole-coefficients in an IIR filter which approximates the signal. By applying an inverse filtering of such a filter we get a source-filter separation where the source is the prediction error, as shown in Figure~\ref{fig:transformation_system}.

LPC\abbrev{LPC}{Linear Prediction Coding} has a number of equivalent representations good properties for different tasks. The representations used in the voice transformation are presented shortly in Section~\ref{sub:cepstrum} and \ref{sub:reflection_coefficients}.

\subsection{Linear Predictive Coding} % (fold)
\label{sub:linear_predictive_coding}
Linear predictive coding, LPC, was first applied to speech signals by Bishnu Atal \etal in 1968 \cite{atal68}. LPC approximates the signal as a $p$-th order all-pole filter by predicting the current sample as a linear combination of its last $p$ samples \cite{digsig}
\begin{equation}
	\tilde{x}(n) = \sum_{k=1}^{p}\alpha_k x(n-k)
\end{equation}
The prediction error by this representation is 
\begin{equation}
	\begin{split}
		e(n)= & x(n)-\tilde{x}(n) \\
		= & x(n)-\sum_{k=1}^{p}\alpha_k x(n-k)
	\end{split}
\end{equation}

The prediction coefficients $a_k$ can be estimated by the minimum mean square error technique, which estimates the coefficients that minimise the total prediction error $E$.
\begin{equation}
	\label{eq:prediction_error}
	\begin{split}
		E = & \sum_{n} e^2(n)\\
		= & \sum_{n}\pbrackets{x(n)-\sum_{k=1}^{p}\alpha_k x(n-k)}^2
	\end{split}
\end{equation}
The coefficients can be obtained by taking the derivative of \eqref{eq:prediction_error} with respect to $a_i$, and equating to 0. By setting $\partial E/\partial \alpha_i = 0$ we obtain
\begin{equation}
	\sum_{n}x(n-i)x(n) = \sum_{k}\alpha \sum_{n}x(n-i)x(n-k), \qquad  1\leq i\leq p 
	\label{eq:lpc_part1}
\end{equation}	
By introducing autocorrelation defined as
\begin{equation}
	R(k) = \sum_{n} x(n) x(n+k)
\end{equation}
Equation \eqref{eq:lpc_part1} becomes
\begin{equation}
	R(i) = \sum_{k=1}^{p} \alpha_k R(\abs{i-k}), \qquad  1\leq i\leq p
	\label{eq:lpc_part2}
\end{equation}
Which yields a set of $p$ linear equations with $p$ unknown parameters, which easily can be solved by \eg the Levinson-Durbin algorithm \cite{digsig,cybenko80}.
\begin{equation}
	\begin{bmatrix}
		R(0) & R(1) & \dots & R(p-1) \\
		R(1) & R(0) & \dots & R(p-2) \\
		\vdots & \vdots & \ddots & \vdots \\
		R(p-1) & R(p-2) & \dots & R(0) \\
	\end{bmatrix} 
	\begin{bmatrix}
		\alpha_1 \\
		\alpha_2 \\
		\vdots \\
		\alpha_p \\
	\end{bmatrix}
	=
	\begin{bmatrix}
		R_1 \\
		R_2 \\
		\vdots \\
		R_p \\
	\end{bmatrix}
	\label{eq:yule_walker}
\end{equation}
% subsection Linear Predictive Coding (end)

\subsection{Cepstrum} % (fold)
\label{sub:cepstrum}
The cepstrum was first introduced by Bogert \etal \cite{bogert63} in 1963. The cepstrum of a signal is a homomorphic transformation to the \emph{quefrency} domain \cite{taletek}. Cepstral coefficients (CC \abbrev{CC}{Cepstral Coefficients}) have the property of low correlation between different frames of coefficients.

The complex cepstrum is defined as the inverse Fourier transform of the log Fourier transform
\begin{equation}
	c(n) = \frac{1}{2\pi}\int_{-\pi}^{\pi} \ln X(e^{j\omega}) e^{j\omega n}d\omega
	\label{eq:complex_cepstrum}
\end{equation}
However, the complex cepstrum $c(n)$ parameters can be converted from LPC coefficients $\alpha_n$ by the following recursion:
\begin{equation}
	c(n) = \begin{cases}
		\alpha_n+{\displaystyle\sum_{k=1}^{n-1} \pbrackets{\frac{k}{n}}} c(k) \alpha_{n-k}, & 0<n\leq p\\
		{\displaystyle\sum_{k=n-p}^{n-1} \pbrackets{\frac{k}{n}}} c(k) \alpha_{n-k}, & n > p\\
	\end{cases}
	\label{eq:ar2cc}
\end{equation}
where $p$ is the number of LPC coefficients. The cepstrum representation is defined with infinity number of coefficients, anything less will be an approximation. The usual choice of accuracy is in the vicinity of 20 coefficients.

The cepstrum parameters can be converted back from LPC by rearranging the equation,
\begin{equation}
	c_n-{\displaystyle\sum_{k=1}^{n-1} \pbrackets{\frac{k}{n}}} c(k) \alpha_{n-k} = \begin{cases}
		\alpha_n, &\quad 0< n \leq p\\
		0, &\quad n > p
	\end{cases}
	\label{eq:cc2ar}
\end{equation}
However, the recursion assumes that the cepstrum parameters represents an all-pole model with poles inside the unit circle, minimum-phase, which might not be the case. Which means that if the cepstrum parameters have been altered, for instance by an transformation function, the resulting LPC parameters might be unstable and the recursion will not yield 0 for $n>p$.  

Another approach to map CC to LPC is to use equation \eqref{eq:complex_cepstrum}. By taking the Fourier transform of the complex cepstrum and the inverse logarithm, we get the Fourier transform of the signal. The magnitude of the Fourier transform yields the power spectrum of the signal. From the power spectrum we can get the autocorrelation of the signal by applying the inverse Fourier transform.
\begin{figure}[htbp]
  \centering
  \begin{tabular}[h]{c}
	\xymatrix{ 
  		c(n) \ar[r] &*+<5mm>[F]{\text{DFT}}\ar[r] &*+<5mm>[F]{\exp(\cdot)}\ar[r] &*+<5mm>[F]{\abs{\cdot}^2} \ar[r] &*+<5mm>[F]{\text{IDFT}} \ar[r] & R_{xx}(n)}    
  \end{tabular}
  \caption{Complex cepstrum to autocorrelation}
  \label{fig:cc2ar}
\end{figure}
From the autocorrelation representation we have the same situation as in \eqref{eq:yule_walker}, which means that we can get the LPC parameters by the Levinson-Durbin algorithm \cite{cybenko80}. The number of DFT coefficients can be chosen arbitrary to get the desired resolution.
% subsection Cepstrum (end)

\subsection{Reflection Coefficients} % (fold)
\label{sub:reflection_coefficients}
Reflection coefficients (RC\abbrev{RC}{Reflection Coefficients}) is another equivalent representation of LPC which is easily checked for stability. The reflection coefficients has a dynamic range of $\cbrackets{-1,1}$ if they are stable. 

The reflection coefficients $k$ can be obtained from the linear prediction coefficients $\alpha$ by the following recursion
\begin{equation}
	\label{eq:ar2rf}
	\begin{split}
		k_i = & \alpha_i^i, \quad i=p,\dotsc,1 \\
		\alpha_j^{i-1} = & \frac{\alpha_j^i+\alpha_j^i \alpha_{i-j}^i}{1-k_i^2}, \quad 1\leq j<i
	\end{split}
\end{equation}
where $\alpha_i^p=\alpha_i$.

The coefficients can be converted back to LP by \cite{taletek}
\begin{equation}
	\label{eq:rf2ar}
	\begin{split}
		\alpha_i^i = & k_i, \quad i=1,\dotsc,p \\
		\alpha_j^i = & \alpha_j^{i-1}-k_i \alpha_{i-j}^{i-1}, \quad 1\leq j<i
	\end{split}
\end{equation}
where $\alpha_i=\alpha_i^p$.
% subsection Reflection Coefficients (end)

\subsection{Pitch Detection} % (fold)
\label{sub:pitch_detection}
Pitch is somewhat the perceptual ``tone'' of the voice and can not directly be measured from the signal. However, the pitch period can be simplified to label the smallest true period of the signal. The inverse of the smallest period of a small analysis interval yields the fundamental frequency $F_0(t)$. If there is no distinct period in the analysis window, it is detected as unvoiced.
Since unvoiced excitation signal is regarded as white noise, there is no point in transforming the unvoiced parts. Used in a global transform together with voiced parts would only degrade the transformation of voiced segments.

There are many methods of prediction the pitch and detecting voicing in a speech signal, in fact, it is written many books on the subject \cite{kleijn95}. They all, however, use some correlation algorithm, with some pre and post processing, to find the smallest distinctive period in the signal.

D. Talkin suggested a method using cross-correlation, in contrast to autocorrelation as used in many other techniques, called robust algorithm for pitch tracking (RAPT\abbrev{RAPT}{Robust Algorithm for Pitch Tracking}) \cite{talkin95}. The cross-correlation in this matter is much the same as autocorrelation, but considers a correlation window of more than just the current analysis frame. Let $x$ be the a windowed signal segment, the cross-correlation $\chi$ of $x$ is defined as
\begin{equation}
	\chi_{i,k} = \sum_{j=m}^{m+n-1}x_j x_{j+k}, \quad k=\sbrackets{0,K-1},m=iz,i=\sbrackets{0,M-1}
\end{equation}
where $i$ is the frame index for $M$ frames, $z$ is the number of sample to advance for each frame, $k$ is the lag index and $n$ is the number of samples in the correlation window.

The RAPT algorithm uses two versions of the speech signal, one at the original sample rate and one with significantly reduced sample rate. The reduced sample rate version is passed into a normalised cross-correlation function (NCCF\abbrev{NCCF}{Normalised Cross-Correlation Function}) and the local maximum correlation distance is recorded. The results of the first iteration is used as a initialisation of the same procedure with the original signal, where the local maxima of the NCCF are restricted to be in the vicinity of the results of the first iteration. The second iteration yields candidate $f_0$ values and are used in a dynamic programming algorithm which utilises certain properties of the NCCF and voiced signal segments. Such properties are \begin{inparaenum}[\itshape a\upshape)] \item if there are more than one local maximum, the shortest period is the correct choice, \item $f_0$ is slowly varying over time so local maxima in adjacent frames should be located at comparable lags, \item if the change in adjacent frames are significant it should be a doubling or a halving of lags, \item a change in voicing in adjacent frames will occur at low frequencies, \item the short-time spectra of voiced and unvoiced frames are significantly different and \item amplitude tend to increase at the onset of voicing and to decrease at offset \cite{talkin95}.\end{inparaenum}

To find the pitch peak of a analysis frame is simple to search for the maximum value in the vicinity of where its supposed to be according to the fundamental frequency results.

 % Simple inverse filter tracking (SIFT\abbrev{SIFT}{simple inverse filter tracking}) is a method for pitch estimation, proposed by John D. Markel in 1972 \cite{markel72}, is a simple method which only utilises concepts already introduced. 

% \TODO{new method, signal representation is F0}
% The fundamental frequency will be lower than 1000 Hz for a normal voice so we are only interested in low frequency part of the signal. First, the speech signal is lowpass-filtered and downsampled to a sampling frequency of 2 kHz. Then the signal is passed through an inverse fourth-order LPC filter, which is sufficient for such a low sampling frequency, to flatten the spectrum. From the excitation from the filtering we compute the short-time autocorrelation and simply detect the highest peak as the pitch period. To enhance the accuracy we can interpolated the autocorrelation in the region of the peak. Moreover, we can detect if the segment is voiced or unvoiced by choosing a minimum level of the normalized autocorrelation peak for the signal to be voiced. This is because the excitation of unvoiced speech can be approximated as white noise which has no correlation.
% \begin{figure}[htbp]
%   \centering
%   \begin{tabular}[h]{c}
% 	\xymatrix{ 
%   		x(n) \ar[r] &*+<5mm>[F]{\textifsym{H|L}}\ar[r] &*+<5mm>[F]{\downarrow}\ar[r] &*+<5mm>[F]{1-A(z)} \ar[r] &*+<5mm>[F]{R_{xx}(\cdot)} \ar[r] &*+<5mm>[F]{\txt{$F_0$ detection}} }    
%   \end{tabular}
%   \caption{Signal manipulation for voiced/unvoiced and $F_0$ detection.}
%   \label{fig:mfcc}
% \end{figure}



% subsection Pitch Prediction (end)


% section Signal Representation (end)


\section{Gaussian Mixture Model} % (fold)
\label{sec:gaussian_mixture_model}
The GMM is a classical parametric model used in many pattern recognition techniques \cite{stylianou98}. Because of the wide variety of speech sound, representing all of them would require enormous complexity of both training data and the transformation function. A probabilistic Gaussian mixture model is a simpler and faster approach. GMM classifies a set of vectors into a smaller number of classes. Instead of representing a class by its mean value, as VQ \cite{gray84}, GMM represents a class as a Gaussian distribution where the distributions can overlap. Vector quantization represents a vector by the mean of the closest class, called hard decision, which could result in large errors. GMM on the other hand uses a soft decision by representing a vector as a sum of weighted Gaussian distributions. A GMM can model a large set of vectors with only a few parameters which there are well defined methods of estimating, \eg estimation maximisation (EM) \cite{taletek}.

The GMM assumes that the probability distribution of the observed parameters takes the following parametric form
\newcommand{\nnn}{\mathcal N}
\begin{equation}
	\label{eq:gmm}
	p(\mathbf{x}) = \sum_{i=1}^{m} \alpha_i \nnn(\mathbf{x}; \boldsymbol{\mu}_i, \mathbf{\Sigma}_i)
\end{equation}
where $m$ is the number of mixture models and $\nnn(\mathbf{x}; \boldsymbol{\mu}_i, \mathbf{\Sigma}_i)$ denotes the p-dimensional normal distribution \cite{statistikk} with the mean vector $\boldsymbol{\mu}_i$ and covariance matrix $\mathbf{\Sigma}_i$ defined by
\begin{equation}
	\nnn(\mathbf{x}; \boldsymbol{\mu}, \mathbf{\Sigma}) = \frac{1}{\sqrt{(2\pi)^p\abs{\mathbf{\Sigma}}}} \exp\sbrackets{-\frac{1}{2} (\mathbf{x} -\boldsymbol{\mu})^T \mathbf{\Sigma}^{-1} (\mathbf{x} -\boldsymbol{\mu})}
\end{equation}
The weighting factor $\alpha_i$ in \eqref{eq:gmm} is the prior probability of class $i$ with constraints $\sum_{i=1}^{M}\alpha_i = 1$ and $\alpha_i \geq 0$. The input vectors $\mathbf{x}_i$ are assumed to be independent \cite{stylianou98}. If the input vector is a joint vector $\mathbf{z}=\sbrackets{\mathbf{x},\mathbf{y}}$ the coveriance matrix will be
\begin{equation}
	\Sigma^z = \begin{bmatrix}
		\Sigma^{xx} & \Sigma^{xy} \\
		\Sigma^{yx} & \Sigma^{yy} \\
	\end{bmatrix}
\end{equation}

The conditional probability that a given observation vector $\mathbf{x}$ belongs to the component $C_i$ of the GMM, is given by Bayes' rule \cite{statistikk}
\begin{equation}
	\label{eq:bayes}
	P(C_i\vert \mathbf{x}) = \frac{\alpha_i \nnn(\mathbf{x}; \boldsymbol{\mu}_i, \mathbf{\Sigma}_i)}{\sum_{j=1}^{m}\alpha_j \nnn(\mathbf{x}; \boldsymbol{\mu}_j, \mathbf{\Sigma}_j)}
\end{equation}

The parameters $\boldsymbol{\alpha}, \boldsymbol{\mu}$ and $ \mathbf{\Sigma}$ can be estimated with the expectation maximisation (EM\abbrev{EM}{Expectation Maximisation}) algorithm. The EM algorithm is an iterative algorithm for unsupervised learning in which component information is unavailable. The EM algorithm tries to find the parameters in the GMM that gives the maximum likelihood of the observed data $\mathbf{X}$. The initial values for the parameters of the GMM can be computed from the VQ representation of the source vectors. The next step is to find the expectation of the log-likelihood of the latent data given the current estimate of the parameters. A new estimate of the parameters is found by maximising the expected log-likelihood and the process is repeated until convergence \cite{taletek}.

% section Gaussian Mixture Model (end)

% \section{Expectation Maximisation} % (fold)
% \label{the:expectation_maximisation}
% We need to determine the parameter $\mathbf{\Phi} = \{\boldsymbol{\alpha}, \boldsymbol{\mu}, \mathbf{\Sigma}\}$ which maximises $P(Y=y\vert \mathbf{\Phi})$ where $y$ is the observed training data. We assume some parameter vector $\mathbf{\Phi}$ and estimate the probability of some unknown data $x$ occurred in the generation of $y$. We then compute a new $\bar{\mathbf{\Phi}}$ which is the maximum likelihood of $\mathbf{\Phi}$ and set the new $\bar{\mathbf{\Phi}}$ to be $\mathbf{\Phi}$ and repeat the process iteratively until the process converges \cite{taletek}. The process will converge if we choose $\bar{\mathbf{\Phi}}$ such that 
% \begin{equation}
% 	\label{eq:q_criteria}
% 	Q(\mathbf{\Phi},\bar{\mathbf{\Phi}})\geq Q(\mathbf{\Phi},\mathbf{\Phi})
% \end{equation}
% where 
% \begin{equation}
% 	\label{eq:q_function}
% 	Q(\mathbf{\Phi},\bar{\mathbf{\Phi}}) = E[\log P(X,Y=y\vert \bar{\mathbf{\Phi}})].
% \end{equation}
% % section Estimation Maximasation (end)

\section{Dynamic Time Warping} % (fold)
\label{sec:dynamic_time_warping}
\newcommand{\ttt}{\mathcal T}
\newcommand{\auto}{R}
\newcommand{\Auto}{\boldsymbol R}

Feature comparison of two signals gives more sense if they are aligned in time. If the only difference in two signals is a time shift, they will not appear as equal if they are not aligned in time. Dynamic time warping (DTW\abbrev{DTW}{Dynamic Time Warping}) is a dynamic programming concept to warp two sequence such that the difference in the signals are minimised. The signals are segmented into small parts which are compared and given a similarity-score. For LP parameters a good similarity-score is the \emph{Itakura distance} \cite{itakura75min},
\newcommand{\ita}{d_{I}}
\begin{definition}
	Given two vectors $\mathbf{x}$ and $\mathbf{y}$ of LP coefficients from a LPC analysis of speech signals $\mathbf{a}$ and $\mathbf{b}$, and the autocorrelation of $\mathbf{a}$, $\auto_{aa}$. The Itakura distance between the two LP vectors is
	\begin{equation}
		\label{eq:itakura_distance}
		\begin{split}
			\ita(\mathbf{x},\mathbf{y}) = & \log\pbrackets{\frac{\mathbf{y}^T \ttt(\auto_{aa}) \mathbf{y}}{\mathbf{x}^T \ttt(\auto_{aa}) \mathbf{x}}} \\
			 = & \log\pbrackets{\mathbf{y}^T \ttt(\auto_{aa}) \mathbf{y}}, \quad x_0 = 1
		\end{split}
	\end{equation}
\end{definition}
where $\ttt$ is the Toeplitz-matrix defined as 
\begin{equation}
	\label{eq:toeplitz}
	\ttt(\auto) = 
	\begin{bmatrix}
		\auto_0 & \auto_1 & \dotsi & \auto_{n-1} 	\\
		\auto_1 & \auto_0 & \dotsi & \auto_{n-2} \\
		\vdots & \vdots & \ddots & \vdots \\
		\auto_{n-1} & \auto_{n-2} & \dotsi & \auto_{0}
	\end{bmatrix}
\end{equation}	

The Itakura distance is not a true metric, however, since it is not symmetric \cite{kreyszig89}. The denominator of \eqref{eq:itakura_distance} is the residual, or the power of the error, from the LP analysis of $\mathbf{a}$ with the source coefficients $\mathbf{x}$. For normalized coefficients, $x_0 = 1$, the power is 1. The numerator is the power of the residual from the LP analysis of $\mathbf{a}$ with the coefficients $\mathbf{y}$. 

The two signals which are to be aligned in time are represented as sequences of feature vectors, $\mathbf{X}=\sbrackets{\mathbf{x}_1,\mathbf{x}_2,\dotsc,\mathbf{x}_N}^T$ and $\mathbf{Y}=\sbrackets{\mathbf{y}_1,\mathbf{y}_2,\dotsc,\mathbf{y}_M}^T$. 

% Equation \eqref{eq:itakura_distance} is applied to every pair of such vectors to compute a full distance matrix
% \begin{equation}
% 	\mathbf{D}(i,j) = \log\pbrackets{\mathbf{y}_j^T \ttt(\auto_{a_i}) \mathbf{y}_j}
% \end{equation}

The minimum accumulated distance between the two signals can be found by choosing the shortest path from a finite set of possible paths. The problem can be solved as a recursion \cite{taletek}
\newcommand{\dmin}{\mathbf{D}_{min}}
\begin{equation}
	\label{eq:dtw_recursion}
	\begin{split}
		\dmin(i,j) = \min \bigl[& \dmin(i-1,j-1)+\alpha \ita(\mathbf{x}_i,\mathbf{y}_j),\\
		& \dmin(i-1,j)+\beta \ita(\mathbf{x}_i,\mathbf{y}_j),\\
		& \dmin(i,j-1)+\beta \ita(\mathbf{x}_i,\mathbf{y}_j),\\
		% & \dmin(i-2,j-1)+\gamma \ita(\mathbf{a}_i,\mathbf{b}_j),\\
		& \dmin(i-1,j-2)+\gamma \ita(\mathbf{x}_i,\mathbf{y}_j)\bigr]		
	\end{split}
\end{equation}
where $\mathbf{x}_i$ and $\mathbf{y}_j$ denotes the LP coefficients of segment $i$ and $j$, respectively, from the two signals. The problem is broken down into simpler subproblems, hence dynamic programming. 

The possible paths are shown in Figure~\ref{fig:dtw_shortest_path}. 
\begin{figure}[htbp]
	\begin{center}
		\setlength{\unitlength}{0.8cm}
		\begin{picture}(8,6.5)(0,0)
		\put(0.2,0.5){\vector(1,0){8}}
		\put(7,0){$\mathbf{B}$}
		\put(-0.4,5){$\mathbf{A}$}
		\put(0.2,0.5){\vector(0,1){5.5}}
		\put(7,5){\circle*{0.3}}
		\put(4,2){\vector(1,1){2.8}}
		\put(4,5){\vector(1,0){2.8}}
		\put(7,2){\vector(0,1){2.8}}
		\put(1,2){\vector(2,1){5.8}}
		% \put(4,1){\vector(1,2){2.9}}
		% \put(5,4){$\gamma$}
		\put(3.7,3.7){$\gamma$}
		\put(7.1,3){$\beta$}
		\put(5,5.1){$\beta$}
		\put(5,3.4){$\alpha$}
		\put(2.6,5){\tiny{$(i,j-1)$}}
		\put(6.6,1.7){\tiny{$(i-1,j)$}}
		\put(0.3,1.7){\tiny{$(i-1,j-2)$}}
		% \put(3.1,0.7){\tiny{$(i-2,j-1)$}}
		\put(3,1.7){\tiny{$(i-1,j-1)$}}
		\end{picture}
		\caption{Possible paths in DTW algorithm.}
		\label{fig:dtw_shortest_path}
	\end{center}
\end{figure}
The weights $\alpha,\beta$ and $\gamma$, called local constraints, are optional and could have a significant effect on the outcome. For instance, the $\beta$ weights can be assigned a larger value to avoid multiple skipping or repetition of frames. Global constraints can be applied in addition to the local constraints which limits the resulting path by borders to guarantee a maximum modification of the signals.

\begin{remark}
There is no option for the path $\dmin(i-2,j-1)$ to $\dmin(i,j)$ to ensure that all segments of the source speaker are used. Moreover, if a source vector is mapped to two or more target vectors, only one of the target vectors, or the mean of the target vectors can be used to represent the mapping. This is important because in source speaker is used as input in the transformation function. For the same reason, the global constraints can not allow open ends for the source vector since this will in practise mean stripping off the edges of the signal.
\end{remark}

The indices in the resulting shortest path are used to duplicate and/or delete frames in the target signal to get a time-aligned version of the target matrix which is the best match to the LP matrix of the source speaker.
% section Dynamic Time Warping (end)



\section{Transformation Function} % (fold)
\label{sec:transformation_function}
Given a set of time-aligned LP vectors of the source and target speaker, $\mathbf{x}$ and $\mathbf{y}$, and a GMM fitted to a joint vector $\mathbf{z}=\sbrackets{\mathbf{x},\mathbf{y}}$, as depicted in Figure~\ref{fig:VC_training_full},
\begin{figure}[htbp]
	\centering
	\begin{tabular}[h]{c}
		\xymatrix{ 
\mathbf{x} \ar[r] &*+<5mm>[F-,]{\txt{LPC}}\ar[d] &\\ % end line 1
&*+<5mm>[F-,]{\txt{DTW}} \ar @<3pt>[rr]^<(0.15){\mathbf{x}} \ar @<-3pt>[rr]_<(0.15){\tilde{\mathbf{y}}} & &*+<5mm>[F-,]{\txt{GMM}}\\ % end line 2
\mathbf{y} \ar[r] &*+<5mm>[F-,]{\txt{LPC}}\ar[u]&&}
	\end{tabular}
	\caption{Training of GMM used in full covariance matrix transformation.}
	\label{fig:VC_training_full}
\end{figure}
we want to define a transformation function which yields the most likely target vector given a new source vector
\newcommand{\fff}{\mathcal F}
\begin{equation}
	\fff(\mathbf{x}) = E[\mathbf{y}\vert \mathbf{x}]
\end{equation}
If we assume that the source and target vectors are jointly Gaussian, the likelihood of $\mathbf{y}$ given $\mathbf{x}$ for a single component GMM can be expressed as \cite{kay93}
\begin{equation}
	E \sbrackets{\mathbf{y}\vert \mathbf{x}} = \boldsymbol{\mu}^y + \mathbf{\Sigma}^{yx} \pbrackets{\mathbf{\Sigma}^{xx}}^{-1} (\mathbf{x}-\boldsymbol{\mu}^x)
\end{equation}
where $E$ denotes the expectation, $\boldsymbol{\mu}$ is the mean vector
\begin{equation}
	\boldsymbol{\mu}^y = E\sbrackets{\mathbf{y}}
\end{equation}
and $\mathbf{\Sigma}$ is the cross-covariance matrix
\begin{equation}
	\mathbf{\Sigma}^{yx} = E \sbrackets{(\mathbf{y}-\boldsymbol{\mu}^y)(\mathbf{x}-\boldsymbol{\mu}^x)^T}
\end{equation}

Stylianou \etal \cite{stylianou95} suggested using this formula for a multicomponent GMM yielding the transformation function
\begin{definition}
	Given a Gaussian mixture model trained with a joint vector of time aligned feature vectors $\mathbf{z} = \sbrackets{\mathbf{x},\mathbf{y}}$ and a new feature vector $\mathbf{x}$ not contained in the training set of the GMM. The most likely correspond feature vector $\mathbf{y}$ can be obtained by
	\begin{equation}
		\label{eq:transformation_function}
		\begin{split}
			\fff(\mathbf{x}) =& E \sbrackets{\mathbf{y}\vert \mathbf{x}}\\
			=& \sum_{i=1}^{m}P(C_i \vert \mathbf{x})\sbrackets{ \boldsymbol{\mu}_i^y + \mathbf{\Sigma}_i^{yx} \pbrackets{\mathbf{\Sigma}_i^{xx}}^{-1} (\mathbf{x}-\boldsymbol{\mu}_i^x)}
		\end{split}
	\end{equation}
\end{definition}

where the likelihoods are weighted with the conditional probabilities of $\mathbf{x}$ belonging to component $C$. 

\subsection{Diagonal Covariance Matrix} % (fold)
\label{sub:diagonal_covariance_matrix}
To decrease the computational complexity, the covariance matrix in the GMM can be constrained to be diagonal, if it is a good assumption with the signal representation in question. Cepstrum is such a representation. However, by constraining the covariance matrix to be diagonal we have to estimate the cross-covariance $\mathbf{\sigma}^{yx}$.

\begin{figure}[htbp]
	\centering
	\begin{tabular}[h]{c}
		\xymatrix{ 
\mathbf{x} \ar[r] &*+<5mm>[F-,]{\txt{LPC}}\ar[d]\ar @{=>}[r] &*+<5mm>[F-,]{\txt{GMM}}\ar `[rd]^{\boldsymbol{\Phi}} [dr] &\\ % end line 1
&*+<5mm>[F-,]{\txt{DTW}} \ar @<3pt>[rr]^<(0.15){\mathbf{x}} \ar @<-3pt>[rr]_<(0.15){\tilde{\mathbf{y}}} & &*+<5mm>[F-,]{\txt{FT training}}\\ % end line 2
\mathbf{y} \ar[r] &*+<5mm>[F-,]{\txt{LPC}}\ar[u]&&}
	\end{tabular}
	\caption{Training of filter transformation parameters with diagonal covariance matrix.}
	\label{fig:VC_training_diag}
\end{figure}

If we train the GMM with $\mathbf{Z}=\mathbf{X}$, the mean $\boldsymbol{\mu}^x$ and variance of the source vector $\boldsymbol{\sigma}^{xx}$ can be derived from the GMM, while the mean of the target vectors $\boldsymbol{\mu}^y$ and cross-covariance matrix $\mathbf{\sigma}^{yx}$ are unknown and need to be estimated. 

Equation \eqref{eq:transformation_function} with diagonal covariance matrices will become
\begin{equation}
	\label{eq:diagonal_transformation}
	\mathcal{F}(x_t^{(k)}) = \sum_{i=1}^{m} P(C_i\vert \mathbf{x}_t) [\mu_i^{y^{(k)}}+\sigma_i^{yx^{(k)}}  (x_t^{(k)}-\mu_i^{x^{(k)}})/\sigma_i^{xx^{(k)}}]
\end{equation}
where $k=1,\dots,p$ and denotes one cepstrum coefficient. \eqref{eq:diagonal_transformation} can be written as a single matrix equation,
\begin{equation}
	\label{eq:least_square_problem}
	\begin{split}
		\mathbf{Y} = &\mathbf{P}\boldsymbol{\mu}^y + \mathbf{D}\mathbf{\Sigma}^{yx} \\
		= & \begin{bmatrix}
			\mathbf{P}& \vdots &\mathbf{D}
		\end{bmatrix}
		\begin{bmatrix}
			\boldsymbol{\mu^y} \\
			\dots \\
			\mathbf{\Sigma}^{yx}
		\end{bmatrix}
	\end{split}
\end{equation}
where $\mathbf{Y}$ is the training set of the target spectral vectors with dimensions $n\times p$, and $\mathbf{P}$ is a $n \times m$ matrix with the posterior probabilities
\begin{equation}
	\label{eq:P_matrix}
	\mathbf{P} = \begin{bmatrix}
		P(C_1\vert \mathbf{x}_1) & \dots & P(C_m\vert \mathbf{x}_1) \\
		P(C_1\vert \mathbf{x}_2) & \dots & P(C_m\vert \mathbf{x}_2) \\
		\vdots & & \vdots \\
		P(C_1\vert \mathbf{x}_n) & \dots & P(C_m\vert \mathbf{x}_n) \\
	\end{bmatrix}
\end{equation}
$\mathbf{D}$ is a $\pbrackets{n \times m}\times p$ matrix defined as
\begin{equation}
	\label{eq:D_matrix_new}
	\mathbf{D}^{(k)} = \begin{bmatrix}
		P(C_1\vert \mathbf{x}_1) (x_1^{(k)} - \mu_1^{x^{(k)}})/\sigma_1^{xx^{(k)}} & \dots & P(C_m\vert \mathbf{x}_1) (x_1^{(k)} - \mu_m^{x^{(k)}})/\sigma_m^{xx^{(k)}} \\
		P(C_1\vert \mathbf{x}_2) (x_2^{(k)} - \mu_1^{x^{(k)}})/\sigma_1^{xx^{(k)}} & \dots & P(C_m\vert \mathbf{x}_2) (x_2^{(k)} - \mu_m^{x^{(k)}})/\sigma_m^{xx^{(k)}} \\
		\vdots & & \vdots \\
		P(C_1\vert \mathbf{x}_n) (x_n^{(k)} - \mu_1^{x^{(k)}})/\sigma_1^{xx^{(k)}} & \dots & P(C_m\vert \mathbf{x}_n) (x_n^{(k)} - \mu_m^{x^{(k)}})/\sigma_m^{xx^{(k)}} \\
	\end{bmatrix}
\end{equation}
The two unknown matrices $\boldsymbol{\mu}^y$ and $\mathbf{\Sigma}^{yx}$ will have dimensions $m\times p$ and $m \times \pbrackets{p\times p}$ 
\begin{equation}
	\label{eq:v_matrix}
	\boldsymbol{\mu}^y = 
	\begin{bmatrix}
		\boldsymbol{\mu}^y_1, \boldsymbol{\mu}^y_2, \dotsc, \boldsymbol{\mu}^y_m
	\end{bmatrix}^T
\end{equation}
\begin{equation}
	\label{gamma_matrix}
	\mathbf{\Sigma}^{yx} = 
	\begin{bmatrix}
		\mathbf{\Sigma}_1^{yx}, \mathbf{\Sigma}_2^{yx}, \dotsc, \mathbf{\Sigma}_m^{yx}
	\end{bmatrix}^T
\end{equation}

We want to find the set $\sbrackets{\boldsymbol{\mu}^y, \mathbf{\sigma}^{yx}}^T$ which minimises  the total squared conversion error
\begin{equation}
	\label{eq:conversion_error}
	\epsilon = \sum_{t} \norm{\mathbf{y}_t - \mathcal{F}(\mathbf{x}_t)}^2
\end{equation}
This least-square problem can be solved by the normal equations \cite{strang06}
\begin{equation}
	\label{eq:param_computed}
	\begin{split}
		\left( 
		\begin{bmatrix}
			\mathbf{P}^T \\
			\dotsi \\
			\mathbf{D}^{(k)^T}
		\end{bmatrix}
		\begin{bmatrix}
			\mathbf{P} & \vdots & \mathbf{D}^{(k)}
		\end{bmatrix}
		 \right)
		\begin{bmatrix}
			\boldsymbol{\mu}^y \\
			\dotsi \\
			\mathbf{\Sigma}^{yx}
		\end{bmatrix}
		= &
		\begin{bmatrix}
			\mathbf{P}^T \\
			\dotsi \\
			\mathbf{D}^{(k)^T}
		\end{bmatrix}
		\mathbf{Y} \\ % end line 1
		\begin{bmatrix}
			\boldsymbol{\mu}^y \\
			\dotsi \\
			\mathbf{\Sigma}^{yx}
		\end{bmatrix}
		= &
		\left( 
		\begin{bmatrix}
			\mathbf{P}^T \\
			\dotsi \\
			\mathbf{D}^{(k)^T}
		\end{bmatrix}
		\begin{bmatrix}
			\mathbf{P} & \vdots & \mathbf{D}^{(k)}
		\end{bmatrix}
		 \right)^{-1}
		\begin{bmatrix}
			\mathbf{P}^T \\
			\dotsi \\
			\mathbf{D}^{(k)^T}
		\end{bmatrix}
		\mathbf{Y} \\ % end line 2
	\end{split}
\end{equation}
% subsection Diagonal Covariance Matrix (end)

% section Transformation Function (end)


\section{Synthesis} % (fold)
\label{sec:synthesis}
Given a transformed set of pitch and frequency feature parameters, we would like to synthesise the new transformed voice. As depicted in Figure~\ref{fig:transformation_system} the excitation from the inverse LP filtering is passed through the PSOLA algorithm which changes the pitch pulse distances to the transformed markings. The transformed excitation is agin passed through a LP synthesise filter with the transformed filter coefficients.

\subsection{PSOLA} % (fold)
\label{sub:psola}
Pitch synchronous overlap-and-add (PSOLA\abbrev{PSOLA}{Pitch Synchronous Overlap-and-Add}) procedure can modify the pace and/or the pitch of a speech signal. Time domain PSOLA splits the signals into two pitch periods long windowed parts and re-synthesises the parts with the overlap-add procedure, by duplicating or deleting frames to alter the pace of the voice and adjusting the spacing of windows to alter the pitch \cite{taletek}. The goal is to alter the pitch of the signal without affecting the spectral characteristics while this is taken care of by the LP parameters in the synthesise filter. 

The unvoiced parts of the excitation signal is approximately white noise and needs no modification. The voiced parts can be represented as a function of pitch cycles
\begin{equation}
	e(n)=\sum_{i=-\infty}^{\infty}e_i(n-t_s(i))
\end{equation}
where $t_s(i)$ are the epochs of the source speaker signal and the pitch period is
\begin{equation}
	P_s(i) = t_s(i)-t_s(i-1)
\end{equation}

One pitch cycle can be represented by applying a Hamming window, $w$, of length two pitch periods, to the signal. The term ``overlap and add'' comes from the use of overlapping windowed segments.
\begin{equation}
	e_i(n)=w_i(n)e(n)
\end{equation}
If we replace the source timing instances, $t_s$, with the ones from the target, $t_t$, we get the desired pitch contour.
\begin{equation}
	\tilde{e}(n)=\sum_{i=-\infty}^{\infty}e_i(n-t_t(i)) % should have been y_j
\end{equation}
The same procedure can be used on unvoiced speech as well if the epoch distance is set uniformly and smaller than 10 ms \cite{moulines95a}.
% subsection PSOLA (end)

% section Synthesis (end)

\section{Robustness} % (fold)
\label{sec:robustness}
To increase the performance of the transformation certain pre-processing techniques can be applied to emphasise signal characteristics important for the transformation.

\subsection{Endpoint Detection} % (fold)
\label{sub:endpoint_detection}
To exclude non-informative information, like silence in the beginning and end of a sentence, could enhance the transformation because it excludes unwanted training data. If the signal to noise ratio is high it is a trivial task. However, if there is some background noise it is not that easy to detect the boundaries of the speech utterance. The basic approach is the derive the energy of the signal and choose a boundary for how much energy there is in the utterance and clip the signal where the energy crosses this boundary. But if there is a peak in the noise level when the speaker is silent, the peak could be recognised as the start of a sentence.

\begin{figure}[htbp]
	\begin{center}
		\includegraphics[width=.75\textwidth]{fig/endpoint_detection}
		\caption{Endpoint detection with utterance boundary ITU and silence boundary ITL \cite{rabiner75}.}
		\label{fig:endpoint_detection}
	\end{center}
\end{figure}
\TODO{replace figure?}

A more robust method of endpoint detection proposed by \cite{rabiner75} is to locate where we know for with a higher probability where this an utterance and backtrace from this point to the silence boundary, illustrated in Figure~\ref{fig:endpoint_detection}. Another technique for avoiding peak-noise error is the use of short-time energy. Instead of calculating the energy in each sample separately we can introduce a memory component to smooth out the energy contour.
\begin{equation}
	E(n)=\sum_{j=0}^{n}\alpha^j x^2(n-j)
	\label{eq:basic_ste}
\end{equation}
where $\alpha < 1$ decides how much information from the previous samples to include in the current energy value. Equation \eqref{eq:basic_ste} can be simplified to
\begin{equation}
	\begin{split}
		E(n+1)=& \sum_{j=0}^{n+1}\alpha^jx^2(n+1-j)\\
		=& x^2(n+1)+\sum_{j=1}^{n+1}\alpha^jx^2(n+1-j)\\
		=& x^2(n+1)+\sum_{k=0}^{n}\alpha^{k+1} x^2(n-k)\\
		=& x^2(n+1)+\alpha E(n), \quad E(0)=x^2(0)\\
	\end{split}
	\label{eq:short_time_energy}
\end{equation}

By estimating the energy of the signal by \eqref{eq:short_time_energy} the endpoints can be detected by choosing a utterance boundary of \eg $ITU=0.02$ and a lower silence boundary, \eg $ITL=0.002$. The algorithm will seek for the first sample which exceeds $ITU$ and backtrace from this sample to the first sample with lower energy than $ITL$.
% subsection Endpoint detection (end)



\subsection{Pre-emphasis} % (fold)
\label{sub:pre_emphasis}
Some implementations of frequency transformation lacks a good transformation of high frequencies \cite{turk06}. A temporarily increase of the energy in high frequencies during the transformation operations could cope with this problem. This can be achieved by passing the speech signals through a pre-emphasis filter in the pre-processing and do the inverse operation before synthesis. The pre-emphasis filter is a simple one state IIR filter
\begin{equation}
	\label{eq:pre_emphasis}
	H(z) = \frac{1}{1-\alpha z^{-1}}
\end{equation}
where $0<\alpha < 1$. The frequency response with $\alpha=0.97$ is depicted in Figure~\ref{fig:pre_emph_097}.

According to \cite{turk06} pre-emphasis yields a significant improvement for sampling frequencies greater than 16 kHz for LSF feature vectors. Furthermore he claims that pre-emphasis relaxes the time-aligned criteria for codebook matching with the transformation input vector. 
\begin{figure}[htbp]
	\begin{center}
		\includegraphics[width=.8\textwidth]{fig/pre_emph_097}
		\caption{Frequency response of a pre-emphasis filter with 
		feedback coefficient $\alpha=0.97$.}
		\label{fig:pre_emph_097}
	\end{center}
\end{figure}
However, wether it provides an increase in the overall quality of the frequency transformation for cepstral coefficients is not thoroughly tested.

% subsection Pre-emphasis (end)
% section Robustness (end)

\section{Objective Result Metrics} % (fold)
\label{sec:objective_metrics}
The best quality metric of a voice transformation is the subjective impression from a listening test. However, it is dependant of the listeners and can be time consuming. Objective metrics could be useful too in the development phase. They are often easy to measure and gives a consequent result if used properly. 

\subsection{Frequency Metrics} % (fold)
\label{sub:frequency_metrics}
Itakura distance \eqref{eq:itakura_distance} is a distance metric to measure the difference between LP vectors. Cepstral distance is another metric useful in measuring the quality of the frequency transformation from the cepstral domain. 
\begin{definition}
	Let $\hat{\mathbf{c}}$ be a vector of transformed cepstral coefficients and $\mathbf{c}^t$ the vector of target coefficients. The cepstral distance between the two vectors is
	\begin{equation}
		\label{eq:cepstral_distance}
		\epsilon(\hat{\mathbf{c}},\mathbf{c}^t) = \sum_{i=1}^{n-1}(\hat{c}_{i}-c_{i}^t)^2
	\end{equation}
	where the energy coefficient $c_0$ is left out.
	
	The Normalised cepstral distance (NCD\abbrev{NCD}{Normalised Cepstral Distance}) of the transformed vector with source vector $\mathbf{c}^s$ is
	\begin{equation}
		\label{eq:ncd}
		\epsilon(\hat{\mathbf{c}},\mathbf{c}^t,\mathbf{c}^s) = \frac{{\sum}_{i=1}^{n-1}(\hat{c}_{i}-c_{i}^t)^2}{\sum_{j=1}^{n-1}(c_{j}^s-c_{j}^t)^2}
	\end{equation}
	Which measures the relative improvement of the transformation.
	% \begin{equation}
	% 	e(\hat{c}^t,c^t) = \frac{\sum_{i=1}^{N}\sum_{j=2}^{13}(\hat{c}_{ij}^t-c_{ij}^t)^2}{\sum_{i=1}^{N}\sum_{j=2}^{13}(c_{ij}^s-c_{ij}^t)^2}
	% \end{equation}	
\end{definition}

While NCD measures the improvement of the transform relative to untransformed source vectors in the cepstral domain, the $L_2$ metric operates in the frequency domain and measures the deviation of the transformed spectrum to the target spectrum.

\begin{definition}
	Given the power spectrum $S(\omega)$ of a signal
\begin{equation}
	S(\omega) = \frac{\sigma^2}{\abs{1-\sum_{i}a_i e^{-j\omega_i}}^2}
\end{equation}
let $\mathbf{a}$ and $\mathbf{b}$ be two signals, the log RMS distortion is
\begin{equation}
	d_{rms}(\mathbf{a},\mathbf{b}) = \frac{1}{2\pi}\int_{-\pi}^{\pi}\left\lvert\ln \frac{S_a(\omega)}{S_b(\omega)}\right\rvert^2 d\omega
\end{equation}
the $L_2$ metric is defined as \cite{gray76}
\begin{equation}
	\label{eq:l2_metric}
	L_2(\mathbf{a},\mathbf{b})  = 4.343 \sqrt{d_{rms}(\mathbf{a},\mathbf{b})} \quad [dB]
\end{equation}
The $L_2$ is a true metric in the sense that they satisfy the triangle inequality in addition to being symmetric and positive definite \cite{kreyszig89}.
\end{definition}

\begin{remark}
	The $L_2$ metric is the root mean square (RMS) log spectral measure in decibel. The factor $4.343$ comes from the conversion from natural logarithm to decibel
	\begin{equation}
		10\log(x) = 10 \frac{\ln(x)}{\ln(10)} = 4.343\ln(x) 
	\end{equation}	
\end{remark}

The $L_2$ metric normalised with the source spectrum is
\begin{equation}
	\begin{split}
	Nd_{rms}(\mathbf{a},\mathbf{b},\mathbf{c}) = & \frac{1}{2\pi}\int_{-\pi}^{\pi} \frac{\abs{\ln S_a(\omega)-\ln S_b(\omega)}^2}{\abs{\ln S_a(\omega)-\ln S_c(\omega)}^2} d\omega \\
	NL_2(\mathbf{a},\mathbf{b},\mathbf{c}) = & 4.343 \sqrt{Nd_{rms}(\mathbf{a},\mathbf{b},\mathbf{c})} \quad [dB]
	\end{split}
\end{equation}
% subsection Frequency metrics (end)

\subsection{Pitch Metrics} % (fold)
\label{sub:pitch_metrics}
The quality of a pitch transformation can be measured by controlling the fundamental frequency $f_0$ of each pitch period. The fundamental frequency is the frequency of pitch marks $pm$, \ie 
\begin{equation}
	f_0(i) = \frac{1}{pm(i)-pm(i-1)}
\end{equation}

For a sentence of several pitch periods the average error in fundamental frequency is
\begin{equation}
	\mu_{\epsilon} = \frac{1}{N}\sum_{i=0}^{N-1} f_0^t(i) - \hat{f_0}(i)
\end{equation}
where $f_0^t$ is the target fundamental frequency and $\hat{f_0}$ is the transformed fundamental frequency.

A metric for relative improvement from the source pitch $f_0^s$ used in this paper is the normalised pitch distance (NPD\abbrev{NPD}{Normalised Pitch Distance})
\begin{equation}
	NPD = \sqrt{\frac{\sum_{i=0}^{N-1} \pbrackets{f_0^t(i)-\hat{f_0}(i)}^2}{\sum_{i=0}^{N-1} \pbrackets{f_0^t(i)-f_0^s(i)}^2}}
\end{equation}

% subsection Pitch metrics (end)

% section Objective result metrics (end)

% chapter Theory (end)